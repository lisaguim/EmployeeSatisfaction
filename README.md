
<h2 align="center"> Predict Employee Satisfaction </h2>

### üìå Project description and methodology

In this project, we will use data science and machine learning techniques to analyze and predict the factors that influence employee satisfaction in a company. The goal is to identify the most significant variables that affect employee well-being and motivation, such as work environment, compensation, growth opportunities, and work-life balance. We will use a dataset that includes several employee characteristics, such as age, length of service, performance reviews, feedback, and satisfaction rates.

### üìú Dataset

The dataset used was extracted from Kaggle and was created by IBM's HR team.

The glossary of the columns is available in the wiki section here on git.

### üìö Libraries

Python libraries used for this project are:

- sklearn
- pandas

### Explory Analysis and target variable



In this step, we checked null & duplicate values in dataset and  positive class prevalence.

### ‚öôÔ∏è Pre-Processing

  - Data Division at trainning, validation and test: 

  The dataset was divided into training, validation and test subsets (70/15/15). These subsets were generated by random sampling (since this is not a time series). A random sample of 30% of the original data was extracted, which was later 
  separated into df_test and df_valid. And the remaining 70% corresponds to df_train. In this way, it is possible to maintain a positive prevalence of ~20% in each subset.

  Subset:
  
     df_train = train machine learning algorithm and represents the majority of dataset volume
     df_test = will be used to adjust hyperparameters and select best perform
     df_valid = will be used to test the accuracy of ML algorithm

  - Class balancing (trainning subset): 

  It was checked that dataset had a balacing class of 80% negative values versus 20% positive values. Therefore, class balancing was applied for trainning dataset through undersampling technique.
  
![All Dataset before class balancing](https://github.com/user-attachments/assets/18ab1366-5e96-4471-ac97-d5c9c17c5453)

  After that, class balancing is 50% and train dataset reduced from 8050 to 3248 rows (samples).

![After class balancing](https://github.com/user-attachments/assets/32db8d91-8ee6-4dae-aecc-9204aa5a9c34)

  - Standardization:

In this step, the data from the columns (features) of the training and validation sets are standardized (or normalized) using the StandardScaler class from the sklearn library.

###  üóëÔ∏è Data Modeling Predictive

  - Metrics:

Metric functions were defined to evaluate the three predictive model options.
Since this is a binary classification model, the metrics used were AUC (Area Under the Curve), Accuracy, Recall, Precision and Specificity.
  
  - Creating mmodels:

When determining the machine learning classification algorithm, three prediction algorithm options will be created: Logistic Regression, Gaussian Naive Bayes, and Decision Tree with Boosting.
The objective of this step is to evaluate which model is best (based on the metrics) for this project.

### üìó Cross Validation

The best model so far is the third option (Gaussian Naive Bayes) based on the metrics evaluations, but it is still necessary to verify its generalization capacity, that is, if the model understood the mathematical relationship between the data and not only the details of the training data. Therefore, in this step, we be used the cross validation to check if model is overfitting or not.

### ‚öñÔ∏è Hypermarameter Optimization

As everything can be improved, this step aims to optimize the xgboosting hyperparameter for model 3, in order to achieve the best model performance through GridSearchCV from sklearn.

### üìä Results

Based on the analysis of the data, the optimized XGBoost prediction model is the best model as it presented the best performance and evaluation in the AUC metric.

![results](https://github.com/user-attachments/assets/6e006577-9b1e-4685-a7ea-e11bf85b1e90)


